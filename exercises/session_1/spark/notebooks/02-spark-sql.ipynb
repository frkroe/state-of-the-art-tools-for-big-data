{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638b4d68",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "This notebook demonstrates how to use Spark SQL to perform data analysis using SQL queries on DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e759026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Exercises\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56f086-e04e-4751-95e9-550aba975d60",
   "metadata": {},
   "source": [
    "## Load and familiarize yourself with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets directory to store our csv file\n",
    "!mkdir -p ../datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1c1c6",
   "metadata": {},
   "source": [
    "Let's prepare the dataset.\n",
    "To load the dataset in our \"datasets\" directory, you have two options:\n",
    "1. **Mount the local data/tickets.csv file** from the repository into your notebook environment in the docker-compose setup.\n",
    "2. **Upload the csv file manually** in this Jupyter environment.\n",
    "\n",
    "You can find the csv file in the repositoy or download it directly from [this link](https://www.kaggle.com/datasets/tobiasbueck/multilingual-customer-support-tickets?select=dataset-tickets-multi-lang3-4k.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1cc5f-8a5f-4888-81b7-fc703b3e64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have uploaded the csv file sucessfully.\n",
    "!ls -lh ../datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb75a7-9fde-46a4-9191-0e64b578141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first 5 rows of the csv file\n",
    "!head -n 5 ../datasets/tickets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a4722-f86c-43ba-b4ba-91fb367beba5",
   "metadata": {},
   "source": [
    "## Read CSV with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aebcf0-3088-41c3-b5d7-675457246bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe from csv file\n",
    "ticketsDF = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"multiLine\", True)\n",
    "        .option(\"escape\", \"\\\"\")                # handle inner quotes\n",
    "        .csv(\"../datasets/tickets.csv\")\n",
    ")\n",
    "\n",
    "# Print schema and first 5 rows\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e5383-9fea-45b6-9e84-3a79cd580d86",
   "metadata": {},
   "source": [
    "## Recap - Basic Filtering, Grouping & Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ee8ba-6587-4f6d-bb88-227c541647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Show only high-priority tickets\n",
    "highPriorityDF = <YOUR CODE HERE>\n",
    "\n",
    "highPriorityDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b6fb1-5947-4a38-b58f-e3a35b211fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Return the number of tickets by type (Incident, Request…), ordering them from biggest amount to least\n",
    "ticketsByTypeDF = <YOUR CODE HERE>\n",
    "\n",
    "ticketsByTypeDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb76fe1-732b-4d3d-b0ff-d528ae3d0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3: Count tickets by language\n",
    "ticketsByLangDF = <YOUR CODE HERE>\n",
    "\n",
    "ticketsByLangDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be34752-c85e-42cc-90d4-e24e51622797",
   "metadata": {},
   "source": [
    "## SQL Exercises\n",
    "\n",
    "In order to use SQL query language, we need to register a table based on our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af1bfd-36e2-460e-8bf6-e9991702346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketsDF.createOrReplaceTempView(\"tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd33cd0-4642-4be1-a3ce-b21ba79da646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 rows\n",
    "spark.sql(\"\"\"\n",
    "<YOUR SQL QUERY HERE>\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13086c5-fb5d-4093-a7e3-7d827f01b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Count tickets by priority (SQL version)\n",
    "<YOUR SQL QUERY HERE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a295ae-d411-42f7-86cb-7a15dc2c5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Which ticket subjects contain the keyword “Account” (SQL version)\n",
    "<YOUR SQL QUERY HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337fd1e-16d5-4fae-8b1c-71152f44d0dd",
   "metadata": {},
   "source": [
    "## UDFs (User-Defined Functions)\n",
    "\n",
    "UDFs (User-Defined Functions) allow you to create custom functions that can be applied to DataFrame columns in Spark SQL. They are useful when you need to perform operations that are not available in the built-in functions provided by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef68ceb-e297-43a3-9bb9-1ba6be20a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Create a UDF to detect whether a ticket is security-related\n",
    "\n",
    "security_keywords = [\"security\", \"cyber\", \"breach\", \"attack\", \"incident\", \"risk\"]\n",
    "\n",
    "# Create a function that checks if any of the security keywords (case insensitive) are present in the subject or body of the ticket\n",
    "def is_security_ticket(subject, body):\n",
    "    <YOUR CODE HERE>\n",
    "    return ...\n",
    "\n",
    "# Register the UDF, specifying the return type as BooleanType - True if security-related, False otherwise\n",
    "isSecurityUDF = udf(is_security_ticket, BooleanType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b874715-1709-41d1-9c58-82ac30ac1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to our tickets DataFrame\n",
    "ticketsSecurityDF = ticketsDF.withColumn(\n",
    "    \"is_security_ticket\",\n",
    "    isSecurityUDF(col(\"subject\"), col(\"body\"))\n",
    ")\n",
    "\n",
    "# Print the results, showing the subject and whether it's a security ticket\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37754598-9d00-4d38-a444-d216c50a9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the security tickets\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d34551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same UDF but on SQL\n",
    "<YOUR SQL QUERY HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538488fe-05c7-44a6-95c1-ac85dccede7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the security tickets with SQL\n",
    "<YOUR SQL QUERY HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318d2e6-179e-4e8f-9a14-d1ba5b461855",
   "metadata": {},
   "source": [
    "## Working with arrays\n",
    "\n",
    "As you might have notices we have multiple columns for tags. Let's convert these tags columns to an array and perform operations on an array column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f52275-eae7-49ef-bd1d-2ec7c2c41b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Create new column \"tags\" of type array for our Dataframe, containing all the tags (tag_1 to tag_8)\n",
    "tagsDF = ticketsDF.withColumn(<YOUR CODE HERE>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478f038-3604-4edf-889a-2f993fe0bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Explode tags to find most common tags\n",
    "explodedTagsDF = tagsDF.select(explode(\"tags\").alias(\"tag\"))\n",
    "explodedTagsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f8391-2d3e-4d14-8110-8bd2ffb03bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tags and print the results from most used to least used, ignoring NULL values\n",
    "tagCountsDF = (\n",
    "    explodedTagsDF.<YOUR CODE HERE>\n",
    ")\n",
    "\n",
    "tagCountsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d1ca3-788e-44cb-bef4-090a8bc43b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
